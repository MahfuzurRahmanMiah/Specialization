{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Foursquare_streaming_practice.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahfuzurRahmanMiah/Specialization/blob/master/Foursquare_streaming_practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjnR3iGe54du",
        "colab_type": "text"
      },
      "source": [
        "Running Your Queries In Spark You need to take the data from Foursquare and perform your analysis based on the question you chose.\n",
        "\n",
        "In our example below, we do the following:\n",
        "\n",
        "We read the files that our Foursquare client generates from the drive. For each city, we get the trending venue categories and the number of people currently being there. We add up the numbers for the same categories. You can extend this into a web dashboard, or plots inside this notebook if you choose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCC-BZqh545F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "161a4711-6dc9-499a-e174-ccb316fcb2ea"
      },
      "source": [
        "# Install Java, Spark, Findspark and PySpark\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://apache.osuosl.org/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.0-bin-hadoop2.7.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\"\n",
        "\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "\n",
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tar: spark-2.4.0-bin-hadoop2.7.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n",
            "\u001b[K     |████████████████████████████████| 217.8MB 59kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 45.9MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=0b0156f6ebd30b453aaf7b92177af48619050e7243f81eb176c0650532f36fb3\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFlCRi2X55M5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import the relevant modules\n",
        "from pyspark import SparkConf,SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "from pyspark.sql import SparkSession"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnVD4PEY6DA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "The code below deletes all the log files inside the foursquare_logs directory\n",
        "import shutil\n",
        "folder = \"/content/gdrive/My Drive/Colab Datasets/foursquare_logs\"\n",
        "for the_file in os.listdir(folder):\n",
        "    file_path = os.path.join(folder, the_file)\n",
        "    try:\n",
        "        if os.path.isfile(file_path):\n",
        "            os.unlink(file_path)\n",
        "        #elif os.path.isdir(file_path): shutil.rmtree(file_path)\n",
        "    except Exception as e:\n",
        "        print(e)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXAfllvM6JBR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def updateFunction(newValues, runningCount):\n",
        "    if runningCount is None:\n",
        "        runningCount = 0\n",
        "    return sum(newValues, runningCount)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B57s8xwt6O6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create spark configuration\n",
        "conf = SparkConf()\n",
        "conf.setAppName(\"FoursquareStreamApp\")\n",
        "\n",
        "# create spark context with the above configuration\n",
        "sc = SparkContext.getOrCreate(conf=conf)\n",
        "sc.setLogLevel(\"ERROR\")\n",
        "\n",
        "# create the Streaming Context from the above spark context with \n",
        "# interval size 10 seconds\n",
        "ssc = StreamingContext(sc,10)\n",
        "\n",
        "# setting a checkpoint to allow RDD recovery\n",
        "ssc.checkpoint(\"checkpoint_FoursquareApp\")\n",
        "\n",
        "# read data from drive\n",
        "dataStream = ssc.textFileStream(\"/content/gdrive/My Drive/Colab Datasets/foursquare_logs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0I0X3tre6RJD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Finally, we implement our primary workflow.\n",
        "#After the implementation of our workflow, we begin the streaming with ssc.start(). \n",
        "# The query stays open until we terminate it (ssc.awaitTermination()).\n",
        "visitor_counts = dataStream.map(lambda x: (x.split(\",\")[0], int(x.split(\",\")[1]))).reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "runningCounts = visitor_counts.updateStateByKey(updateFunction)\n",
        "\n",
        "runningCounts.pprint()\n",
        "\n",
        "# start the streaming computation\n",
        "ssc.start()\n",
        "\n",
        "# wait for the streaming to finish\n",
        "ssc.awaitTermination()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TJkB1Ew6hc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# When the running process halts, you may need to stop the current \n",
        "# Spark Context by running the following cell:\n",
        "ssc.stop()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}